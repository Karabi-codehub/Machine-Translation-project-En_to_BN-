{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karabi-codehub/Machine-Translation-project-En_to_BN-/blob/main/Capstone_Project_Machine_Translation(English_to_Bangla)_using_Pretrained_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-17T16:12:10.789559Z",
          "start_time": "2025-09-17T16:11:30.818516Z"
        },
        "collapsed": true,
        "id": "5EhCypevPens",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "-For type hints\n",
        "-Any means a variable or return type can be any data type (string, int, tensor, etc.).\n",
        "\"\"\"\n",
        "from typing import Any\n",
        "\n",
        "import os\n",
        "\n",
        "!pip install -U mlflow\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Defines step output types for Lightning training\n",
        "-In PyTorch Lightning, some functions (like training_step, validation_step, test_step) return outputs that can be different types\n",
        "-Lightning gives you a ready-made type alias called STEP_OUTPUT.\n",
        "           -STEP_OUTPUT = shorthand type hint for “whatever the training/validation/test step is allowed to return in Lightning.”\n",
        "           -PyTorch Lightning makes writing and training deep learning models easier and cleaner.\n",
        "           -It handles things like training loops, validation, logging, GPU/TPU support, etc., so you don’t have to write them manually.\n",
        "\"\"\"\n",
        "!pip install pytorch-lightning\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-High-level training framework to simplify PyTorch code\n",
        "\"\"\"\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Core PyTorch library for tensors and computations\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Build neural network layers and models\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Handle custom datasets and batch loading\n",
        "\"\"\"\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Load and preprocess dataset (CSV, Excel, etc.)\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-BLEUScore measures how good a machine translation is by comparing it to reference translations.\n",
        "-BLEU (Bilingual Evaluation Understudy): a metric that checks how closely your model’s translations match the correct/reference translations.\n",
        "-torchmetrics.text.BLEUScore: makes it easy to calculate BLEU in PyTorch projects.\n",
        "-BLEU tells how accurate your translations are.”\n",
        "\"\"\"\n",
        "from torchmetrics.text import BLEUScore\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-AutoTokenizer: Converts text into numbers (tokens) the model can understand.\n",
        "-AutoModelForSeq2SeqLM: Pretrained sequence-to-sequence model for tasks like translation or summarization.\n",
        "-Tokenizer prepares text, Model translates or generates text.\n",
        "\"\"\"\n",
        "%pip install -q transformers datasets sentencepiece\n",
        "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "!pip install -U mlflow pyngrok pytorch-lightning gradio sacremoses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OInbezuP7gY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "-torch.cuda.is_available() → checks if your computer has a GPU ready.\n",
        "-\"cuda\" → runs computations on GPU (faster).\n",
        "-\"cpu\" → runs on the processor if no GPU is available.\n",
        "-torch.device(...) → tells PyTorch where to run your model and tensors.\n",
        "\"\"\"\n",
        "# Chooses GPU (cuda) if available, otherwise CPU, for running your PyTorch model.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnN2fES7QQ0-"
      },
      "outputs": [],
      "source": [
        "mt_pretrained_model_name = \"csebuetnlp/banglat5_nmt_en_bn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CngANDSoQJjt"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "✅ AutoModelForSeq2SeqLM is:Automatic Model for Sequence-to-Sequence Language Modeling\n",
        "\n",
        "Let’s break it down:\n",
        "\n",
        "AutoModel → “Auto” means it automatically picks the right pretrained model architecture (you just give the model name,\n",
        "             e.g., \"t5-small\", \"facebook/mbart-large-50\", etc.).\n",
        "Seq2Seq → stands for Sequence-to-Sequence (input sequence → output sequence).\n",
        "          Used in tasks like translation, summarization, and text generation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAyVoUwoQOaD"
      },
      "outputs": [],
      "source": [
        "\"\"\"A custom dataset class for Machine Translation (MT).\"\"\"\n",
        "#MTDataset → Loads data from a CSV file (your custom dataset).\n",
        "MAX_LENGTH = 128\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, csv_file): # __init__: called when we create the dataset object.\n",
        "        self.data=pd.read_csv(csv_file) #loads the CSV file\n",
        "    def __len__(self): # total number of samples in the dataset.\n",
        "        return len(self.data)\n",
        "    def __getitem__(self,idx): # __getitem__: fetches one sample (source + target) by index.\n",
        "        src_text = str(self.data.iloc[idx]['en']) # Source text (English) from column 'en'\n",
        "        tgt_text = str(self.data.iloc[idx]['bn']) # Target text (Bangla) from column 'bn'\n",
        "        src_encoding=tokenizer(\n",
        "            src_text, # Input sentence (English)\n",
        "            max_length=MAX_LENGTH,  # integer,Maximum length of tokens (fixed size input)\n",
        "            padding='max_length',\n",
        "            truncation=True, # Cuts off text longer than max_length\n",
        "            return_tensors='pt'# Returns PyTorch tensors instead of plain lists\n",
        "         )\n",
        "\n",
        "        tgt_encoding = tokenizer(\n",
        "        tgt_text,              # The target sentence (Bangla text) that we want the model to generate.\n",
        "        max_length=MAX_LENGTH ,        # Fixes the size of the sequence (like saying \"all sentences must be 128 tokens long\").\n",
        "        padding='max_length',  # If the sentence is shorter, add [PAD] tokens until it’s 128 tokens.\n",
        "        truncation=True, # Cut off extra words if the sentence is longer than 128 tokens.\n",
        "        return_tensors='pt'    # Converts everything into PyTorch tensors, ready for training.\n",
        "    )\n",
        "        return {\n",
        "    'src_input_ids': src_encoding['input_ids'].squeeze(0),        # Token IDs for source (English) sentence\n",
        "    'src_attention_mask': src_encoding['attention_mask'].squeeze(0),  # Mask for source (1 = real token, 0 = padding)\n",
        "    'tgt_input_ids': tgt_encoding['input_ids'].squeeze(0),        # Token IDs for target (Bangla) sentence\n",
        "    'tgt_attention_mask': tgt_encoding['attention_mask'].squeeze(0)   # Mask for target\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "example: How are you, dude?\n",
        "input_ids: 125, 14, 145, 78\n",
        "max_length = 7\n",
        "input_ids: [125, 14, 145, 147, 0, 0, 0]\n",
        "attention_mask: [1, 1, 1, 1, 0, 0, 0],src_attention_mask → Mask to ignore [PAD] tokens.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qiH1f8sQjAl"
      },
      "outputs": [],
      "source": [
        "# DataModule definition\n",
        "class MTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_csv, val_csv, test_csv, batch_size=32):\n",
        "        super().__init__() # Call parent LightningDataModule __init__\n",
        "        # Save CSV file paths\n",
        "        self.train_csv = train_csv\n",
        "        self.val_csv = val_csv\n",
        "        self.test_csv = test_csv\n",
        "        # Save batch size (how many samples per batch)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "      # This method prepares datasets for train, val, test\n",
        "    def setup(self, stage=None):\n",
        "        # Create dataset objects using the CSV paths\n",
        "        self.train_dataset = MTDataset(self.train_csv)\n",
        "        self.val_dataset = MTDataset(self.val_csv)\n",
        "        self.test_dataset = MTDataset(self.test_csv)\n",
        "\n",
        "    # DataLoader for training (shuffle=True so model sees random data order every epoch)\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    # DataLoader for validation (shuffle=False so order is fixed)\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "    # DataLoader for testing (also no shuffle)\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "#  Create DataModule object\n",
        "data_module = MTDataModule(\n",
        "    train_csv='train.csv',   # path to training data CSV\n",
        "    val_csv='val.csv',       # path to validation data CSV\n",
        "    test_csv='test.csv',     # path to testing data CSV\n",
        "    batch_size=32            # how many samples per batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtF--xKtQoIk"
      },
      "outputs": [],
      "source": [
        "# Machine Translation Model\n",
        "class MTModel(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load a pretrained Seq2Seq model\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "        # Load tokenizer for the same model (T5-specific)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "        #Define learning rate (small value because we are fine-tuning a pretrained model)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Define loss function (CrossEntropyLoss)\n",
        "        # \"ignore_index=pad_token_id\" means padding tokens won't be counted in loss.\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
        "\n",
        "        # Define evaluation metric (BLEU Score)\n",
        "        # BLEU checks how close translations are to the target sentences.\n",
        "        self.bleu = BLEUScore()\n",
        "\n",
        "    # Forward pass: how the model processes one batch of data\n",
        "    def forward(self,\n",
        "                src_input_ids,        # English tokens\n",
        "                src_attention_mask,   # Mask for English (ignore PAD tokens)\n",
        "                tgt_input_ids,        # Bangla tokens\n",
        "                tgt_attention_mask    # Mask for Bangla\n",
        "        ):\n",
        "        #Call the underlying HuggingFace seq2seq model\n",
        "        outputs = self.model(\n",
        "            input_ids=src_input_ids,                # Source sentence (English)\n",
        "            attention_mask=src_attention_mask,      # Mask for English\n",
        "            decoder_input_ids=tgt_input_ids[:, :-1],# Decoder input (Bangla shifted right, teacher forcing)\n",
        "            decoder_attention_mask=tgt_attention_mask[:, :-1] # Mask for Bangla\n",
        "        )\n",
        "        return outputs   # Contains logits (predicted token probabilities)\n",
        "\n",
        " # Training loop: runs for every batch during training\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.compute_loss(batch, batch_idx, 'train')   # Compute loss\n",
        "        self.log('train_loss', loss, prog_bar=True)           # Log train loss on progress bar\n",
        "        return loss\n",
        "\n",
        "# Validation loop: runs after each epoch on validation data\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.compute_loss(batch, batch_idx, 'val')     # Compute validation loss\n",
        "        self.log('val_loss', loss, prog_bar=True)             # Log validation loss\n",
        "        return loss\n",
        "\n",
        "# Test loop: runs on test data\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self.compute_loss(batch, batch_idx, 'test')    # Compute test loss\n",
        "        self.log('test_loss', loss, prog_bar=True)            # Log test loss\n",
        "        return loss\n",
        "\n",
        " # Optimizer + Scheduler setup\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        # Use AdamW optimizer (works well with transformers)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Use learning rate scheduler (Cosine Annealing: decreases LR smoothly)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=10   # Number of epochs to restart cycle\n",
        "        )\n",
        "\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
        "\n",
        "# ⚡ Compute loss + BLEU (shared by train/val/test)\n",
        "    def compute_loss(self, batch, batch_idx, stage):\n",
        "        # Unpack batch\n",
        "        src_input_ids = batch['src_input_ids']\n",
        "        src_attention_mask = batch['src_attention_mask']\n",
        "        tgt_input_ids = batch['tgt_input_ids']\n",
        "        tgt_attention_mask = batch['tgt_attention_mask']\n",
        "\n",
        "        # Forward pass through model\n",
        "        outputs = self(\n",
        "            src_input_ids,\n",
        "            src_attention_mask,\n",
        "            tgt_input_ids,\n",
        "            tgt_attention_mask\n",
        "        )\n",
        "\n",
        "        # Get predicted token logits (probabilities before softmax)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        #Compute CrossEntropy loss\n",
        "        # Shift target tokens by one position (teacher forcing)\n",
        "        loss = self.loss_fn(\n",
        "            logits.view(-1, logits.size(-1)), # Predictions: flatten for all tokens\n",
        "            tgt_input_ids[:, 1:].contiguous().view(-1) # Targets: shifted right by 1\n",
        "        )\n",
        "\n",
        "        #If validation/test → also compute BLEU score\n",
        "        if stage == 'val' or stage == 'test':\n",
        "            preds = torch.argmax(logits, dim=-1)   # Pick highest probability tokens\n",
        "            pred_texts = self.tokenizer.batch_decode(preds, skip_special_tokens=True)   # Convert to text\n",
        "            tgt_texts = self.tokenizer.batch_decode(tgt_input_ids[:, 1:], skip_special_tokens=True)\n",
        "\n",
        "            # Compute BLEU score (higher = better translation quality)\n",
        "            bleu_score = self.bleu(pred_texts, [[tgt] for tgt in tgt_texts])\n",
        "\n",
        "            # Log BLEU score to progress bar\n",
        "            self.log(f'{stage}_bleu', bleu_score, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxlhekIfTkYV"
      },
      "outputs": [],
      "source": [
        "# Initialize Machine Translation model\n",
        "model = MTModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTTcp1ZNnFbe"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    save_top_k=1,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "checkpoint_path = os.path.join(\n",
        "    os.getcwd(), \"checkpoints\", \"best_model.pth\"\n",
        ")\n",
        "if not os.path.exists(os.path.dirname(checkpoint_path)):\n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLs58FhmTqky"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs = 2,\n",
        "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu',\n",
        "    devices = 1,\n",
        "    precision = 32,\n",
        "    log_every_n_steps = 10,\n",
        "    val_check_interval = 0.25,\n",
        "    callbacks = [checkpoint_callback, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(\"model:\", type(model))\n",
        "    print(\"tokenizer:\", type(tokenizer))\n",
        "    print(\"✅ Both found — ready to save.\")\n",
        "except NameError as e:\n",
        "    print(\"❌ Model or tokenizer not found — re-run your training or loading code first.\")\n"
      ],
      "metadata": {
        "id": "Pqxq3YceoT8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -----------------------------\n",
        "# 1️⃣ Create save folder\n",
        "# -----------------------------\n",
        "save_dir = \"/content/mt_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Save Hugging Face model + tokenizer\n",
        "# -----------------------------\n",
        "model.model.save_pretrained(save_dir)   # Save the inner HF model\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ Save PyTorch weights (.pt)\n",
        "# -----------------------------\n",
        "torch.save(model.model.state_dict(), \"/content/mt_model_weights.pt\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Zip the model folder\n",
        "# -----------------------------\n",
        "shutil.make_archive(\"/content/mt_model\", 'zip', save_dir)\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ Move files to Google Drive\n",
        "# -----------------------------\n",
        "shutil.move(\"/content/mt_model.zip\", \"/content/drive/MyDrive/mt_model.zip\")\n",
        "shutil.move(\"/content/mt_model_weights.pt\", \"/content/drive/MyDrive/mt_model_weights.pt\")\n",
        "\n",
        "print(\"✅ Model folder and weights saved successfully to Google Drive!\")\n",
        "print(\"📁 Check your Google Drive → MyDrive → mt_model.zip and mt_model_weights.pt\")\n"
      ],
      "metadata": {
        "id": "6faFnfemogXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfon2kpDoTD5"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if folder exists before zipping\n",
        "if os.path.exists(\"mt_model\"):\n",
        "    shutil.make_archive(\"mt_model\", 'zip', \"mt_model\")\n",
        "    print(\"✅ Folder zipped successfully.\")\n",
        "else:\n",
        "    print(\"❌ Folder 'mt_model' not found — check your path.\")\n",
        "\n",
        "# Move files to Google Drive\n",
        "if os.path.exists(\"mt_model.zip\"):\n",
        "    shutil.move(\"mt_model.zip\", \"/content/drive/MyDrive/mt_model.zip\")\n",
        "    print(\"✅ mt_model.zip saved to Google Drive.\")\n",
        "else:\n",
        "    print(\"⚠️ mt_model.zip not found after zipping.\")\n",
        "\n",
        "# If you have model weights, move them too\n",
        "if os.path.exists(\"mt_model_weights.pt\"):\n",
        "    shutil.move(\"mt_model_weights.pt\", \"/content/drive/MyDrive/mt_model_weights.pt\")\n",
        "    print(\"✅ mt_model_weights.pt saved to Google Drive.\")\n",
        "else:\n",
        "    print(\"⚠️ mt_model_weights.pt not found.\")\n",
        "\n",
        "\n",
        "print(\"✅ Files saved to your Google Drive (MyDrive folder).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE4e1y-0ocgr"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "#MLflow Tracking\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "mlflow.set_experiment(\"English-Bangla-Translation\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IoeQgoUDQNN"
      },
      "outputs": [],
      "source": [
        "data_module = MTDataModule(\"train.csv\", \"val.csv\", \"test.csv\", batch_size=BATCH_SIZE)\n",
        "model = MTModel(learning_rate=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdIqcgJqohLe"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from mlflow.models import infer_signature\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- TRAIN + LOG ---------------- #\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "    mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
        "    mlflow.log_param(\"epochs\", EPOCHS)\n",
        "\n",
        "    # Train & test\n",
        "    trainer.fit(model=model, datamodule=data_module)\n",
        "    evaluation_score = trainer.test(model=model, dataloaders=data_module.test_dataloader())\n",
        "    mlflow.log_metric(\"test_loss\", evaluation_score[0]['test_loss'])\n",
        "\n",
        "    # Make sample input & output\n",
        "    sample_batch = next(iter(data_module.test_dataloader()))\n",
        "    sample_input = {\n",
        "        'src_input_ids': sample_batch['src_input_ids'],\n",
        "        'src_attention_mask': sample_batch['src_attention_mask']\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        sample_output = model(\n",
        "            sample_input['src_input_ids'].to(model.device),\n",
        "            sample_input['src_attention_mask'].to(model.device),\n",
        "            sample_batch['tgt_input_ids'].to(model.device),\n",
        "            sample_batch['tgt_attention_mask'].to(model.device)\n",
        "        ).logits\n",
        "        model.train()\n",
        "\n",
        "    sample_input_np = {k: v.cpu().numpy().tolist() for k, v in sample_input.items()}\n",
        "    sample_output_np = sample_output.cpu().numpy().tolist()\n",
        "\n",
        "    # Save model to MLflow\n",
        "    signature = infer_signature(sample_input_np, sample_output_np)\n",
        "    mlflow.pytorch.log_model(\n",
        "        pytorch_model=model,\n",
        "        artifact_path=\"mt_model\",\n",
        "        signature=signature,\n",
        "        input_example=sample_input_np\n",
        "    )\n",
        "\n",
        "    RUN_ID = run.info.run_id\n",
        "    print(\"✅ Your MLflow run ID:\", RUN_ID)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fbM_w3bAcL0"
      },
      "outputs": [],
      "source": [
        "   # ---------------- START MLFLOW UI ---------------- #\n",
        "!mlflow ui --port 5000 &>/dev/null &\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBwB72cQo8Ha"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"🔗 MLflow UI URL:\", public_url.public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok4xJ3TwIz7a"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeJs5-6MI5CM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ---------------- CONFIG ---------------- #\n",
        "mt_pretrained_model_name = \"csebuetnlp/banglat5_nmt_en_bn\"  # base architecture\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# ---------------- LOAD TOKENIZER ---------------- #\n",
        "tokenizer = AutoTokenizer.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "# ---------------- LOAD MODEL + YOUR WEIGHTS ---------------- #\n",
        "# Load the base pretrained model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "# Load your fine-tuned weights (must be in the same folder as app.py)\n",
        "state_dict = torch.load(\"mt_model_weights.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict, strict=False)  # strict=False = ignore extra keys\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- TRANSLATION FUNCTION ---------------- #\n",
        "def translate_english_to_bangla(sentence: str) -> str:\n",
        "    input_ids = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_tokens = model.generate(\n",
        "            input_ids,\n",
        "            max_length=MAX_LENGTH,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "# ---------------- GRADIO INTERFACE ---------------- #\n",
        "gr.Interface(\n",
        "    fn=translate_english_to_bangla,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Enter English sentence here...\", label=\"English Text\"),\n",
        "    outputs=gr.Textbox(label=\"Bangla Translation\"),\n",
        "    title=\"English → Bangla Translator (Fine-tuned)\",\n",
        "    description=\"Translates English into Bangla using your fine-tuned model weights.\"\n",
        ").launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHfnmEg5e5Yk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuAxegZRRoTF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}