{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karabi-codehub/Machine-Translation-project-En_to_BN-/blob/main/Capstone_Project_Machine_Translation(English_to_Bangla)_using_Pretrained_Model_with_code_drescription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09b2df2-3f61-47fc-9d68-9ed4635cbd32",
      "metadata": {
        "id": "a09b2df2-3f61-47fc-9d68-9ed4635cbd32"
      },
      "source": [
        "<span style=\"color:green; font-size:18px; font-weight:500;\">\n",
        "Objectives:\n",
        "\n",
        "1. End-to-end machine translation training pipeline   \n",
        "2. Fine-tune a pre-trained model for the custom dataset\n",
        " </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21559dc6-5cc6-45d6-8653-9fc187f81507",
      "metadata": {
        "id": "21559dc6-5cc6-45d6-8653-9fc187f81507"
      },
      "source": [
        "<span style=\"color:#dc2626; font-size:24px; font-weight:700;\">\n",
        "Project 3 : Machine Translation using Pretrained Model\n",
        " </span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5bdac3-bb9f-487d-8c3c-adff371b4ed9",
      "metadata": {
        "id": "2e5bdac3-bb9f-487d-8c3c-adff371b4ed9"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ Fine-Tuning\n",
        " </span>\n",
        "\n",
        "**Fine-tuning** means taking a **pre-trained model** (already trained on a large general dataset) and then training it a little more on your **custom dataset** so it can adapt to your specific task.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A model trained on millions of English‚ÄìFrench sentences can already *translate*.  \n",
        "If you fine-tune it on **medical text translations**, it will become better at translating **medical terms**.  \n",
        "\n",
        "**‚úÖ Why We Use Fine-Tuning**\n",
        "- **Saves time & resources** ‚Äì Training from scratch needs huge data and computation. Fine-tuning reuses the pre-trained model‚Äôs knowledge.  \n",
        "- **Better accuracy** ‚Äì The model already understands general language patterns; fine-tuning adapts it to your domain (e.g., legal, medical, tech).  \n",
        "- **Works with smaller datasets** ‚Äì You don‚Äôt need millions of samples; a smaller domain-specific dataset is enough.  \n",
        "- **Custom specialization** ‚Äì Makes the model good at your *specific* task (translation, sentiment, Q&A, etc.) instead of general tasks only.  \n",
        "\n",
        "üëâ **In short:**  \n",
        "**Fine-tuning = faster, cheaper, and more accurate way to make AI models work for your needs.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0db719-a32d-4a18-99dc-3f2b2a465a45",
      "metadata": {
        "id": "8b0db719-a32d-4a18-99dc-3f2b2a465a45"
      },
      "source": [
        "<span style=\"color:Navy; font-size:20px; font-weight:600;\">\n",
        "Package installation\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "051ddaaf-30ec-49fe-a207-4c133afef2a2",
      "metadata": {
        "id": "051ddaaf-30ec-49fe-a207-4c133afef2a2"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeaa3c7d-7677-4d0b-8756-548661b889b3",
      "metadata": {
        "id": "aeaa3c7d-7677-4d0b-8756-548661b889b3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "-For type hints\n",
        "-Any means a variable or return type can be any data type (string, int, tensor, etc.).\n",
        "\"\"\"\n",
        "from typing import Any\n",
        "\n",
        "\"\"\"\n",
        "# Install MLflow (if not already installed or to upgrade to latest)\n",
        "# The exclamation mark (!) runs a shell command from the notebook cell\n",
        "\"\"\"\n",
        "!pip install -U mlflow\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Import MLflow into Python\n",
        "\"\"\"\n",
        "import mlflow\n",
        "\n",
        "\"\"\"\n",
        "# Import the infer_signature utility from MLflow\n",
        "# This is useful for automatically inferring input/output schema\n",
        "# when logging models\n",
        "\"\"\"\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Defines step output types for Lightning training\n",
        "-In PyTorch Lightning, some functions (like training_step, validation_step, test_step) return outputs that can be different types\n",
        "-Lightning gives you a ready-made type alias called STEP_OUTPUT.\n",
        "           -STEP_OUTPUT = shorthand type hint for ‚Äúwhatever the training/validation/test step is allowed to return in Lightning.‚Äù\n",
        "           -PyTorch Lightning makes writing and training deep learning models easier and cleaner.\n",
        "           -It handles things like training loops, validation, logging, GPU/TPU support, etc., so you don‚Äôt have to write them manually.\n",
        "\"\"\"\n",
        "!pip install pytorch-lightning\n",
        "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-High-level training framework to simplify PyTorch code\n",
        "\"\"\"\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Core PyTorch library for tensors and computations\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Build neural network layers and models\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Handle custom datasets and batch loading\n",
        "\"\"\"\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-Load and preprocess dataset (CSV, Excel, etc.)\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-BLEUScore measures how good a machine translation is by comparing it to reference translations.\n",
        "-BLEU (Bilingual Evaluation Understudy): a metric that checks how closely your model‚Äôs translations match the correct/reference translations.\n",
        "-torchmetrics.text.BLEUScore: makes it easy to calculate BLEU in PyTorch projects.\n",
        "-BLEU tells how accurate your translations are.‚Äù\n",
        "\"\"\"\n",
        "from torchmetrics.text import BLEUScore\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-AutoTokenizer: Converts text into numbers (tokens) the model can understand.\n",
        "-AutoModelForSeq2SeqLM: Pretrained sequence-to-sequence model for tasks like translation or summarization.\n",
        "-Tokenizer prepares text, Model translates or generates text.\n",
        "-Install NLP-related libraries silently (transformers, datasets, sentencepiece)\n",
        "- %pip is the preferred magic in Jupyter notebooks (keeps it scoped to the current kernel)\n",
        "\"\"\"\n",
        "%pip install -q transformers datasets sentencepiece\n",
        "\n",
        "\"\"\"\n",
        "-Import the tokenizer and model classes from Hugging Face Transformers\n",
        "-(Here we‚Äôre using T5Tokenizer and a generic Seq2Seq model as an example)\n",
        "\"\"\"\n",
        "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\"\"\"\n",
        " Install/update additional tools:\n",
        "- mlflow: for experiment tracking\n",
        "- pyngrok: to expose local web apps to the internet (useful for Gradio)\n",
        "- pytorch-lightning: for easier training loops\n",
        "- gradio: to create quick web interfaces\n",
        "- sacremoses: tokenizer/preprocessing library often used with Hugging Face models\n",
        "\"\"\"\n",
        "!pip install -U mlflow pyngrok pytorch-lightning gradio sacremoses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed40f0f-96e3-4272-8f58-d231c68a276d",
      "metadata": {
        "id": "3ed40f0f-96e3-4272-8f58-d231c68a276d"
      },
      "source": [
        "<span style=\"color:Navy; font-size:20px; font-weight:600;\">\n",
        "CPU to GPU Transform\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb46f22-e964-4203-81c9-a4accc30ac3c",
      "metadata": {
        "id": "dfb46f22-e964-4203-81c9-a4accc30ac3c"
      },
      "source": [
        "**Why GPU is preferred:**\n",
        "\n",
        "-CPU is slower for deep learning, especially with large models or datasets.\n",
        "\n",
        "-Neural network training and large tensor operations are much faster on GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3b92fa-7746-4193-88a8-268e7b1aa7ca",
      "metadata": {
        "id": "5c3b92fa-7746-4193-88a8-268e7b1aa7ca"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "-torch.cuda.is_available() ‚Üí checks if your computer has a GPU ready.\n",
        "-\"cuda\" ‚Üí runs computations on GPU (faster).\n",
        "-\"cpu\" ‚Üí runs on the processor if no GPU is available.\n",
        "-torch.device(...) ‚Üí tells PyTorch where to run your model and tensors.\n",
        "\"\"\"\n",
        "# Chooses GPU (cuda) if available, otherwise CPU, for running your PyTorch model.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb25278-0c8a-49b4-ac0d-519c53efc738",
      "metadata": {
        "id": "efb25278-0c8a-49b4-ac0d-519c53efc738"
      },
      "source": [
        "<span style=\"color:Navy; font-size:20px; font-weight:600;\">\n",
        "Task: English to Bangla\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f10017-9be1-496c-9f92-ac5fe0ea4cdb",
      "metadata": {
        "id": "81f10017-9be1-496c-9f92-ac5fe0ea4cdb"
      },
      "source": [
        "<span style=\"color:blue; font-size:20px; font-weight:600;\">\n",
        "üëâ pre tarined model initialization\n",
        "</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bc1344-7455-4ec7-853f-8de1ef8f989f",
      "metadata": {
        "id": "39bc1344-7455-4ec7-853f-8de1ef8f989f"
      },
      "outputs": [],
      "source": [
        "mt_pretrained_model_name = \"csebuetnlp/banglat5_nmt_en_bn\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c3c1b3-8d67-4653-a200-792c728302c6",
      "metadata": {
        "id": "23c3c1b3-8d67-4653-a200-792c728302c6"
      },
      "source": [
        "<span style=\"color:blue; font-size:20px; font-weight:600;\">\n",
        "üëâ pre tarined Tokenizer and model initialization\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34f546f-88b9-4d05-a0a5-73f32c098a89",
      "metadata": {
        "id": "c34f546f-88b9-4d05-a0a5-73f32c098a89"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ Tokenizer & Model in NLP\n",
        " </span>\n",
        "\n",
        "For NLP tasks, we usually need **two main parts**:\n",
        "\n",
        "**1Ô∏è‚É£ Tokenizer**\n",
        "\n",
        "* **What it does:** Converts text (words/sentences) into numbers (tokens) the model can understand.\n",
        "* **Example:**\n",
        "\n",
        "  * Sentence: `\"I love AI\"`\n",
        "  * Tokenizer ‚Üí `[101, 1045, 2293, 9937, 102]` (IDs for words)\n",
        "   Here \"I\"= 101, \" \"=1045, \"love\"=2293, \" \"=9937, \"AI\"=102\n",
        "\n",
        "üëâ Think of it as a **translator from text ‚Üí numbers**.\n",
        "\n",
        "**2Ô∏è‚É£ Model**\n",
        "\n",
        "* **What it does:** Takes those numbers (tokens) and predicts or generates new tokens (output).\n",
        "* **Example (Translation task):**\n",
        "\n",
        "  * Input tokens = `\"I love AI\"`\n",
        "  * Model output tokens = `\"J'adore l'IA\"` (French translation)\n",
        "\n",
        "üëâ Think of it as the **brain that learns and generates answers**.\n",
        "\n",
        "* `tokenizer` ‚Üí prepares the text.\n",
        "* `model` ‚Üí does the actual task (e.g., translation, summarization, etc.).\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "* **Tokenizer = Text ‚Üí Numbers**\n",
        "* **Model = Numbers ‚Üí Predictions/Text**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091070c2-c468-4494-9fcb-24c6c2b384b1",
      "metadata": {
        "id": "091070c2-c468-4494-9fcb-24c6c2b384b1"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(mt_pretrained_model_name)\n",
        "mt_pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "\"\"\"\n",
        "‚úÖ AutoModelForSeq2SeqLM is:Automatic Model for Sequence-to-Sequence Language Modeling\n",
        "\n",
        "Let‚Äôs break it down:\n",
        "\n",
        "AutoModel ‚Üí ‚ÄúAuto‚Äù means it automatically picks the right pretrained model architecture (you just give the model name,\n",
        "             e.g., \"t5-small\", \"facebook/mbart-large-50\", etc.).\n",
        "Seq2Seq ‚Üí stands for Sequence-to-Sequence (input sequence ‚Üí output sequence).\n",
        "          Used in tasks like translation, summarization, and text generation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f78fcc4-009d-45a5-8f89-cd7bb54e0536",
      "metadata": {
        "id": "3f78fcc4-009d-45a5-8f89-cd7bb54e0536"
      },
      "source": [
        "<span style=\"color:blue; font-size:20px; font-weight:700;\">üëâ Data Pipeline </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82da1fd0-ceae-410a-a0a9-85df6d90abc9",
      "metadata": {
        "id": "82da1fd0-ceae-410a-a0a9-85df6d90abc9"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">Class 1 (Dataset):</span> Custom **PyTorch Dataset** Class for Machine Translation (English ‚Üí Bangla).  \n",
        "\n",
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">Class 2 (Module):</span> **PyTorch Lightning DataModule** for Machine Translation (English ‚Üí Bangla).  \n",
        "\n",
        "\n",
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">Tokenizer:<span>\n",
        "Converts text into **token IDs** (numbers).  \n",
        "\n",
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">Dataset class (MTDataset)-</span>Uses the tokenizer to prepare **samples (English ‚Üí Bangla)**.  \n",
        "\n",
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">DataModule class (MTDataModule)-</span> Wraps **train/val/test datasets + DataLoaders**,  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0fb616-2671-4c2c-857d-eb7199848991",
      "metadata": {
        "id": "1c0fb616-2671-4c2c-857d-eb7199848991"
      },
      "source": [
        "<span style=\"color:darkslateblue; font-size:18px; font-weight:600;\">\n",
        "    \n",
        "‚úîÔ∏è Class 1 (Dataset):Custom PyTorch Dataset Class for Machine Translation (English ‚Üí Bangla).  \n",
        "‚Üí Reads CSV and uses **Tokenizer** to convert text ‚Üí tokens.\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60743357-5514-46e1-a86e-505012deb442",
      "metadata": {
        "id": "60743357-5514-46e1-a86e-505012deb442"
      },
      "source": [
        "\n",
        "**Sentence: How are you, dude?**\n",
        "\n",
        "**Tokens:** 'How', 'are', 'you', 'dude?'\n",
        "\n",
        "**ids:** 125, 14, 145, 78\n",
        "\n",
        "**max_length =** 3\n",
        "\n",
        "**ids: [125, 14, 145]**\n",
        "\n",
        "**MTDataset**\n",
        "- Loads data from a **CSV file** (your custom dataset).\n",
        "- Each row has:  \n",
        "  - **en** ‚Üí English text (source sentence)  \n",
        "  - **bn** ‚Üí Bangla text (target sentence)  \n",
        "- Uses a **tokenizer** to convert text ‚Üí tokens (numbers).  \n",
        "- Returns:\n",
        "  - `src_input_ids` ‚Üí tokens for English  \n",
        "  - `src_attention_mask` ‚Üí tells model which tokens are padding  \n",
        "  - `tgt_input_ids` ‚Üí tokens for Bangla  \n",
        "  - `tgt_attention_mask` ‚Üí mask for Bangla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc1b509b-e9f2-4a24-bc9a-78ee9dfcba5a",
      "metadata": {
        "id": "dc1b509b-e9f2-4a24-bc9a-78ee9dfcba5a"
      },
      "outputs": [],
      "source": [
        "\"\"\"A custom dataset class for Machine Translation (MT).\"\"\"\n",
        "#MTDataset ‚Üí Loads data from a CSV file (your custom dataset).\n",
        "MAX_LENGTH = 128\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, csv_file): # __init__: called when we create the dataset object.\n",
        "        self.data=pd.read_csv(csv_file) #loads the CSV file\n",
        "    def __len__(self): # total number of samples in the dataset.\n",
        "        return len(self.data)\n",
        "    def __getitem__(self,idx): # __getitem__: fetches one sample (source + target) by index.\n",
        "        src_text = str(self.data.iloc[idx]['en']) # Source text (English) from column 'en'\n",
        "        tgt_text = str(self.data.iloc[idx]['bn']) # Target text (Bangla) from column 'bn'\n",
        "        src_encoding=tokenizer(\n",
        "            src_text, # Input sentence (English)\n",
        "            max_length=MAX_LENGTH,  # integer,Maximum length of tokens (fixed size input)\n",
        "            padding='max_length',\n",
        "            truncation=True, # Cuts off text longer than max_length\n",
        "            return_tensors='pt'# Returns PyTorch tensors instead of plain lists\n",
        "         )\n",
        "\n",
        "        tgt_encoding = tokenizer(\n",
        "        tgt_text,              # The target sentence (Bangla text) that we want the model to generate.\n",
        "        max_length=MAX_LENGTH ,        # Fixes the size of the sequence (like saying \"all sentences must be 128 tokens long\").\n",
        "        padding='max_length',  # If the sentence is shorter, add [PAD] tokens until it‚Äôs 128 tokens.\n",
        "        truncation=True, # Cut off extra words if the sentence is longer than 128 tokens.\n",
        "        return_tensors='pt'    # Converts everything into PyTorch tensors, ready for training.\n",
        "    )\n",
        "        return {\n",
        "    'src_input_ids': src_encoding['input_ids'].squeeze(0),        # Token IDs for source (English) sentence\n",
        "    'src_attention_mask': src_encoding['attention_mask'].squeeze(0),  # Mask for source (1 = real token, 0 = padding)\n",
        "    'tgt_input_ids': tgt_encoding['input_ids'].squeeze(0),        # Token IDs for target (Bangla) sentence\n",
        "    'tgt_attention_mask': tgt_encoding['attention_mask'].squeeze(0)   # Mask for target\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "example: How are you, dude?\n",
        "input_ids: 125, 14, 145, 78\n",
        "max_length = 7\n",
        "input_ids: [125, 14, 145, 147, 0, 0, 0]\n",
        "attention_mask: [1, 1, 1, 1, 0, 0, 0],src_attention_mask ‚Üí Mask to ignore [PAD] tokens.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0279663b-f3fe-4dfc-bb5d-5585287b02c2",
      "metadata": {
        "id": "0279663b-f3fe-4dfc-bb5d-5585287b02c2"
      },
      "source": [
        "<span style=\"color:blue; font-size:18px; font-weight:600;\">\n",
        "‚úîÔ∏è Class 2 (DataModule):PyTorch Lightning DataModule for Machine Translation (English ‚Üí Bangla).  \n",
        "‚Üí Organizes train/val/test datasets and provides DataLoaders (no tokenization here).\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffec670d-8076-4477-a6c9-2fa926a56a3d",
      "metadata": {
        "id": "ffec670d-8076-4477-a6c9-2fa926a56a3d"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ What the DataModule does here </span>\n",
        "\n",
        "* Think of it as a **data manager**  for PyTorch Lightning.\n",
        "* It organizes your data into **train**, **validation**, and **test** sets.\n",
        "* It tells PyTorch **how to load the data** in batches.\n",
        "\n",
        "**In your case (`MTDataModule`):**\n",
        "\n",
        "1. **setup()** ‚Üí Reads CSV files and creates datasets (`MTDataset`).\n",
        "2. **train\\_dataloader()** ‚Üí Gives batches of training data (shuffled).\n",
        "3. **val\\_dataloader()** ‚Üí Gives batches of validation data (not shuffled).\n",
        "4. **test\\_dataloader()** ‚Üí Gives batches of test data (not shuffled).\n",
        "\n",
        "üëâ In short:\n",
        "**DataModule = One place that prepares & serves data in the dataset for training, validation, and testing.** üöÄ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d4e73c-431b-471c-8ff6-b49d365aeec6",
      "metadata": {
        "id": "86d4e73c-431b-471c-8ff6-b49d365aeec6"
      },
      "source": [
        "**MTDataModule**\n",
        "- Wraps **datasets** + **dataloaders** together (so Lightning can use them easily).  \n",
        "\n",
        "Functions:\n",
        "- `setup()` ‚Üí creates train, validation, and test datasets.  \n",
        "- `train_dataloader()` ‚Üí returns training data in batches (shuffled).  \n",
        "- `val_dataloader()` ‚Üí returns validation data in batches (not shuffled).  \n",
        "- `test_dataloader()` ‚Üí returns test data in batches (not shuffled).\n",
        "\n",
        "**data_module = MTDataModule(...)**\n",
        "- Creates a **DataModule object** with:\n",
        "  - train CSV  \n",
        "  - val CSV  \n",
        "  - test CSV  \n",
        "  - batch size (e.g., 32 samples per batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae8487d-dee3-4c35-a0eb-5db5f076ecfe",
      "metadata": {
        "id": "aae8487d-dee3-4c35-a0eb-5db5f076ecfe"
      },
      "outputs": [],
      "source": [
        "# DataModule definition\n",
        "class MTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_csv, val_csv, test_csv, batch_size=32):\n",
        "        super().__init__() # Call parent LightningDataModule __init__\n",
        "        # Save CSV file paths\n",
        "        self.train_csv = train_csv\n",
        "        self.val_csv = val_csv\n",
        "        self.test_csv = test_csv\n",
        "        # Save batch size (how many samples per batch)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "      # This method prepares datasets for train, val, test\n",
        "    def setup(self, stage=None):\n",
        "        # Create dataset objects using the CSV paths\n",
        "        self.train_dataset = MTDataset(self.train_csv)\n",
        "        self.val_dataset = MTDataset(self.val_csv)\n",
        "        self.test_dataset = MTDataset(self.test_csv)\n",
        "\n",
        "    # DataLoader for training (shuffle=True so model sees random data order every epoch)\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    # DataLoader for validation (shuffle=False so order is fixed)\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "    # DataLoader for testing (also no shuffle)\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "#  Create DataModule object\n",
        "data_module = MTDataModule(\n",
        "    train_csv='train.csv',   # path to training data CSV\n",
        "    val_csv='val.csv',       # path to validation data CSV\n",
        "    test_csv='test.csv',     # path to testing data CSV\n",
        "    batch_size=32            # how many samples per batch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456cd9f6-3b31-4799-b1a5-2c777aadc158",
      "metadata": {
        "id": "456cd9f6-3b31-4799-b1a5-2c777aadc158"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ Machine Translation Data Flow (with PyTorch Lightning)\n",
        "</span>\n",
        "\n",
        "**1. CSV File (train.csv, val.csv, test.csv)**\n",
        "- **Used for:** Storing raw bilingual text (English + Bangla).\n",
        "- **Why:** Keeps data organized and easy to load.\n",
        "\n",
        "**2. `MTDataset`**\n",
        "- **Used for:** Reading a CSV file and preparing samples.\n",
        "- **Why:** Converts each row (English ‚Üí Bangla) into tokenized input/output tensors so the model can understand.\n",
        "\n",
        "**3. `MTDataModule`**\n",
        "- **Used for:** Combining all datasets (train/val/test) + making DataLoaders.\n",
        "- **Why:** PyTorch Lightning expects a `DataModule` to organize and feed data consistently.\n",
        "\n",
        "**4. ‚ö° `setup()`**\n",
        "- **Used for:** Creating train, validation, and test datasets.\n",
        "- **Why:** Prepares data once so loaders can fetch it anytime.\n",
        "\n",
        "**5. `train_dataloader()`**\n",
        "- **Used for:** Returning batches of training data.\n",
        "- **Why:** The model learns from shuffled mini-batches to generalize better.\n",
        "\n",
        "**6. `val_dataloader()`**\n",
        "- **Used for:** Returning validation data batches.\n",
        "- **Why:** Check how well the model is performing on unseen data (no shuffle).\n",
        "\n",
        "**7. `test_dataloader()`**\n",
        "- **Used for:** Returning test data batches.\n",
        "- **Why:** Evaluate final model performance on completely unseen data.\n",
        "\n",
        "**8. `data_module = MTDataModule(...)`**\n",
        "- **Used for:** Creating the actual data module object with paths + batch size.\n",
        "- **Why:** This object is given to the Lightning `Trainer` so it knows **where and how to get the data**.\n",
        "\n",
        "‚úÖ In short:\n",
        "- **MTDataset** = how to read one CSV file into tokenized samples.  \n",
        "- **MTDataModule** = organizes multiple datasets + loaders for training/validation/testing.  \n",
        "- **data_module** = the actual object you will give to your Lightning Trainer.\n",
        "  \n",
        "‚úÖ **CSV ‚Üí Dataset ‚Üí DataModule ‚Üí DataLoader ‚Üí Model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83c1f58-b6a5-42d6-bae1-89320d8b31b8",
      "metadata": {
        "id": "c83c1f58-b6a5-42d6-bae1-89320d8b31b8"
      },
      "source": [
        "<span style=\"color:blue; font-size:20px; font-weight:700;\">üëâ Model </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c61ee6b-5101-4b6a-881a-036592009c94",
      "metadata": {
        "id": "9c61ee6b-5101-4b6a-881a-036592009c94"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ Flowchart: Working Process of MTModel\n",
        "</span>\n",
        "\n",
        "A[Start Training / Validation / Testing] --> B[Load Pretrained Seq2Seq Model + Tokenizer]\n",
        "\n",
        "B --> C[Input English Sentence ‚Üí Tokenizer ‚Üí src_input_ids, src_attention_mask]\n",
        "\n",
        "C --> D[Input Bangla Sentence (Target) ‚Üí Tokenizer ‚Üí tgt_input_ids, tgt_attention_mask]\n",
        "\n",
        "\n",
        "D --> E[Forward Pass (self.forward)]\n",
        "\n",
        "E --> F[Encoder Processes English Input]\n",
        "\n",
        "F --> G[Decoder Processes Bangla Input (Teacher Forcing: tgt_input_ids shifted right)]\n",
        "\n",
        "\n",
        "G --> H[Model Outputs Logits (Predicted Token Probabilities)]\n",
        "\n",
        "H --> I[Compute CrossEntropy Loss (ignore PAD tokens)]\n",
        "\n",
        "\n",
        "I --> J{Stage?}\n",
        "\n",
        "J -->|train| K[Log Train Loss ‚Üí Backpropagation ‚Üí Optimizer Update (AdamW)]\n",
        "\n",
        "J -->|val/test| L[Compute Predictions ‚Üí Decode to Text ‚Üí BLEU Score]\n",
        "\n",
        "L --> M[Log Validation/Test Loss + BLEU Score]\n",
        "\n",
        "\n",
        "K --> N[Learning Rate Scheduler (Cosine Annealing)]\n",
        "\n",
        "M --> N\n",
        "\n",
        "N --> O[Repeat for Next Batch/Epoch]\n",
        "\n",
        "O --> P[End Training/Validation/Testing]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbca2234-e56b-4057-ad67-079ffd24691a",
      "metadata": {
        "id": "fbca2234-e56b-4057-ad67-079ffd24691a"
      },
      "outputs": [],
      "source": [
        "# Machine Translation Model\n",
        "class MTModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load a pretrained Seq2Seq model\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "        #Load tokenizer for the same model\n",
        "        # This converts text ‚Üî tokens (numbers).\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "        #Define learning rate (small value because we are fine-tuning a pretrained model)\n",
        "        self.learning_rate = 2e-5\n",
        "\n",
        "        # Define loss function (CrossEntropyLoss)\n",
        "        # \"ignore_index=pad_token_id\" means padding tokens won't be counted in loss.\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
        "\n",
        "        # Define evaluation metric (BLEU Score)\n",
        "        # BLEU checks how close translations are to the target sentences.\n",
        "        self.bleu = BLEUScore()\n",
        "\n",
        "    # Forward pass: how the model processes one batch of data\n",
        "    def forward(self,\n",
        "                src_input_ids,        # English tokens\n",
        "                src_attention_mask,   # Mask for English (ignore PAD tokens)\n",
        "                tgt_input_ids,        # Bangla tokens\n",
        "                tgt_attention_mask    # Mask for Bangla\n",
        "        ):\n",
        "        #Call the underlying HuggingFace seq2seq model\n",
        "        outputs = self.model(\n",
        "            input_ids=src_input_ids,                # Source sentence (English)\n",
        "            attention_mask=src_attention_mask,      # Mask for English\n",
        "            decoder_input_ids=tgt_input_ids[:, :-1],# Decoder input (Bangla shifted right, teacher forcing)\n",
        "            decoder_attention_mask=tgt_attention_mask[:, :-1] # Mask for Bangla\n",
        "        )\n",
        "        return outputs   # Contains logits (predicted token probabilities)\n",
        "\n",
        " # Training loop: runs for every batch during training\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.compute_loss(batch, batch_idx, 'train')   # Compute loss\n",
        "        self.log('train_loss', loss, prog_bar=True)           # Log train loss on progress bar\n",
        "        return loss\n",
        "\n",
        "# Validation loop: runs after each epoch on validation data\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.compute_loss(batch, batch_idx, 'val')     # Compute validation loss\n",
        "        self.log('val_loss', loss, prog_bar=True)             # Log validation loss\n",
        "        return loss\n",
        "\n",
        "# Test loop: runs on test data\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self.compute_loss(batch, batch_idx, 'test')    # Compute test loss\n",
        "        self.log('test_loss', loss, prog_bar=True)            # Log test loss\n",
        "        return loss\n",
        "\n",
        " # Optimizer + Scheduler setup\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        # Use AdamW optimizer (works well with transformers)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Use learning rate scheduler (Cosine Annealing: decreases LR smoothly)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=10   # Number of epochs to restart cycle\n",
        "        )\n",
        "\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
        "\n",
        "# ‚ö° Compute loss + BLEU (shared by train/val/test)\n",
        "    def compute_loss(self, batch, batch_idx, stage):\n",
        "        # Unpack batch\n",
        "        src_input_ids = batch['src_input_ids']\n",
        "        src_attention_mask = batch['src_attention_mask']\n",
        "        tgt_input_ids = batch['tgt_input_ids']\n",
        "        tgt_attention_mask = batch['tgt_attention_mask']\n",
        "\n",
        "        # Forward pass through model\n",
        "        outputs = self(\n",
        "            src_input_ids,\n",
        "            src_attention_mask,\n",
        "            tgt_input_ids,\n",
        "            tgt_attention_mask\n",
        "        )\n",
        "\n",
        "        # Get predicted token logits (probabilities before softmax)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        #Compute CrossEntropy loss\n",
        "        # Shift target tokens by one position (teacher forcing)\n",
        "        loss = self.loss_fn(\n",
        "            logits.view(-1, logits.size(-1)), # Predictions: flatten for all tokens\n",
        "            tgt_input_ids[:, 1:].contiguous().view(-1) # Targets: shifted right by 1\n",
        "        )\n",
        "\n",
        "        #If validation/test ‚Üí also compute BLEU score\n",
        "        if stage == 'val' or stage == 'test':\n",
        "            preds = torch.argmax(logits, dim=-1)   # Pick highest probability tokens\n",
        "            pred_texts = self.tokenizer.batch_decode(preds, skip_special_tokens=True)   # Convert to text\n",
        "            tgt_texts = self.tokenizer.batch_decode(tgt_input_ids[:, 1:], skip_special_tokens=True)\n",
        "\n",
        "            # Compute BLEU score (higher = better translation quality)\n",
        "            bleu_score = self.bleu(pred_texts, [[tgt] for tgt in tgt_texts])\n",
        "\n",
        "            # Log BLEU score to progress bar\n",
        "            self.log(f'{stage}_bleu', bleu_score, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac56e89-d419-462d-b204-5914b629d5cb",
      "metadata": {
        "id": "3ac56e89-d419-462d-b204-5914b629d5cb"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ Class Model (What & Why)</span>\n",
        "\n",
        "#### Model & Tokenizer\n",
        "- **AutoModelForSeq2SeqLM** ‚Üí Pretrained translation model (already knows how to translate).\n",
        "- **AutoTokenizer** ‚Üí Converts text ‚Üî tokens (needed for model input/output).\n",
        "\n",
        "#### Loss & Metrics\n",
        "- **Loss function (CrossEntropyLoss)** ‚Üí Tells model how wrong predictions are (ignores PAD tokens).\n",
        "- **BLEU Score** ‚Üí Measures how good the translations are (translation quality metric).\n",
        "\n",
        "#### Training Logic\n",
        "- **training_step / validation_step / test_step** ‚Üí Defines what happens in training, validation, and testing.\n",
        "- **compute_loss** ‚Üí Central function where **loss and BLEU** are calculated together.\n",
        "\n",
        "#### Optimization\n",
        "- **Optimizer (AdamW)** ‚Üí Updates model weights during training.\n",
        "- **Scheduler (CosineAnnealingLR)** ‚Üí Smoothly adjusts learning rate (helps stable training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2e308f-876f-4dbe-ba39-f727ba81138c",
      "metadata": {
        "id": "7a2e308f-876f-4dbe-ba39-f727ba81138c"
      },
      "outputs": [],
      "source": [
        "# Initialize Machine Translation model\n",
        "model = MTModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WCjm9_BsRFyp",
      "metadata": {
        "id": "WCjm9_BsRFyp"
      },
      "source": [
        "**Add MLflow Logger**\n",
        "\n",
        "Why: To track experiments (loss, BLEU), save checkpoints, and manage versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3nX3Uc7fRQ98",
      "metadata": {
        "id": "3nX3Uc7fRQ98"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "\n",
        "# Stop training early if validation loss doesn't improve for 2 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Save only the best model (highest val_loss in this case ‚Äî usually use 'min')\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    save_top_k=1,\n",
        "    mode='max'  # use 'min' if you want the lowest loss\n",
        ")\n",
        "\n",
        "# Create a path to save the best model checkpoint\n",
        "checkpoint_path = os.path.join(os.getcwd(), \"checkpoints\", \"best_model.pth\")\n",
        "\n",
        "# Make sure the directory exists before saving the checkpoint\n",
        "if not os.path.exists(os.path.dirname(checkpoint_path)):\n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66809e09-9adc-45a0-8171-c8fd4de5d0db",
      "metadata": {
        "id": "66809e09-9adc-45a0-8171-c8fd4de5d0db"
      },
      "source": [
        "<span style=\"color:blue; font-size:20px; font-weight:700;\">üëâ Train </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76175392-7940-444b-892c-5f657a491100",
      "metadata": {
        "id": "76175392-7940-444b-892c-5f657a491100"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(   # PyTorch Lightning Trainer: controls training/validation/testing loop\n",
        "    max_epochs=3,       # Train the model for 2 full passes over the dataset\n",
        "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "                        # Use GPU if available, otherwise fallback to CPU\n",
        "    devices=1,          # Number of devices (GPUs or CPUs) to use (here = 1 device)\n",
        "    precision=32,       # Use mixed precision (32-bit) to speed up training & use less memory\n",
        "    log_every_n_steps=10,   # Log training metrics (loss etc.) every 10 steps\n",
        "    val_check_interval=0.25, # Run validation 4 times per epoch (after every 25% of the data)\n",
        "    callbacks = [checkpoint_callback, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afb980c-dc8e-43fe-82fd-ecbb2178c5da",
      "metadata": {
        "id": "3afb980c-dc8e-43fe-82fd-ecbb2178c5da"
      },
      "source": [
        "<span style=\"color:Tomato; font-size:18px; font-weight:700;\">\n",
        "üîÑ Trainer Parameters (What & Why) </span>\n",
        "\n",
        "- **Trainer** ‚Üí Automates the whole training loop (no need to manually write epochs, batches, backprop).\n",
        "\n",
        "#### Training Duration\n",
        "- **max_epochs** ‚Üí How long to train (number of full dataset passes).\n",
        "\n",
        "#### Hardware\n",
        "- **accelerator + devices** ‚Üí Pick GPU if available, else CPU (controls where training runs).\n",
        "\n",
        "#### Precision\n",
        "- **precision=16** ‚Üí Mixed precision (faster training, less memory usage).\n",
        "\n",
        "#### Logging & Validation\n",
        "- **log_every_n_steps** ‚Üí How often logs are recorded.\n",
        "- **val_check_interval** ‚Üí How often validation runs (can be multiple times per epoch).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d429be02-5f5b-4ab4-a757-66850a815618",
      "metadata": {
        "id": "d429be02-5f5b-4ab4-a757-66850a815618"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.save(model.state_dict(), 'mt_model_weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d449790a-f26a-474f-a150-ae2d1f13702a",
      "metadata": {
        "id": "d449790a-f26a-474f-a150-ae2d1f13702a"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('mt_model_weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f69de2d-eba5-42c7-b01e-25055e7b900a",
      "metadata": {
        "id": "7f69de2d-eba5-42c7-b01e-25055e7b900a"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "#MLflow Tracking\n",
        "\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "mlflow.set_experiment(\"English-Bangla-Translation\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78409836-7f1a-443d-bb98-d37306158829",
      "metadata": {
        "id": "78409836-7f1a-443d-bb98-d37306158829"
      },
      "outputs": [],
      "source": [
        "data_module = MTDataModule(\"train.csv\", \"val.csv\", \"test.csv\", batch_size=BATCH_SIZE)\n",
        "model = MTModel(learning_rate=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4d12b9-e81f-411b-a1d4-eda5d045dafa",
      "metadata": {
        "id": "1d4d12b9-e81f-411b-a1d4-eda5d045dafa"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from mlflow.models import infer_signature\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- TRAIN + LOG ---------------- #\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "    mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
        "    mlflow.log_param(\"epochs\", EPOCHS)\n",
        "\n",
        "    # Train & test\n",
        "    trainer.fit(model=model, datamodule=data_module)\n",
        "    evaluation_score = trainer.test(model=model, dataloaders=data_module.test_dataloader())\n",
        "    mlflow.log_metric(\"test_loss\", evaluation_score[0]['test_loss'])\n",
        "\n",
        "    # Make sample input & output\n",
        "    sample_batch = next(iter(data_module.test_dataloader()))\n",
        "    sample_input = {\n",
        "        'src_input_ids': sample_batch['src_input_ids'],\n",
        "        'src_attention_mask': sample_batch['src_attention_mask']\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        sample_output = model(\n",
        "            sample_input['src_input_ids'].to(model.device),\n",
        "            sample_input['src_attention_mask'].to(model.device),\n",
        "            sample_batch['tgt_input_ids'].to(model.device),\n",
        "            sample_batch['tgt_attention_mask'].to(model.device)\n",
        "        ).logits\n",
        "        model.train()\n",
        "\n",
        "    sample_input_np = {k: v.cpu().numpy().tolist() for k, v in sample_input.items()}\n",
        "    sample_output_np = sample_output.cpu().numpy().tolist()\n",
        "\n",
        "    # Save model to MLflow\n",
        "    signature = infer_signature(sample_input_np, sample_output_np)\n",
        "    mlflow.pytorch.log_model(\n",
        "        pytorch_model=model,\n",
        "        artifact_path=\"mt_model\",\n",
        "        signature=signature,\n",
        "        input_example=sample_input_np\n",
        "    )\n",
        "\n",
        "    RUN_ID = run.info.run_id\n",
        "    print(\"‚úÖ Your MLflow run ID:\", RUN_ID)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9a7fee-70db-44c5-a459-19a30b5caf1b",
      "metadata": {
        "id": "4f9a7fee-70db-44c5-a459-19a30b5caf1b"
      },
      "outputs": [],
      "source": [
        "   # ---------------- START MLFLOW UI ---------------- #\n",
        "!mlflow ui --port 5000 &>/dev/null &\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cadc10bf-6e7f-4486-912c-c0346e15b95a",
      "metadata": {
        "id": "cadc10bf-6e7f-4486-912c-c0346e15b95a"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"üîó MLflow UI URL:\", public_url.public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037ea324-34ea-4a07-9da4-d827f129025f",
      "metadata": {
        "id": "037ea324-34ea-4a07-9da4-d827f129025f"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40053dd7-4501-424c-8027-558e16547055",
      "metadata": {
        "id": "40053dd7-4501-424c-8027-558e16547055"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "import mlflow.pytorch\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "# ---------------- CONFIG ---------------- #\n",
        "mt_pretrained_model_name = \"csebuetnlp/banglat5_nmt_en_bn\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# ---------------- LOAD TOKENIZER ---------------- #\n",
        "tokenizer = T5Tokenizer.from_pretrained(mt_pretrained_model_name)\n",
        "\n",
        "# ---------------- LOAD MODEL FROM MLFLOW ---------------- #\n",
        "RUN_ID = \"f5ec797e21654b46a383cfc582813440\"\n",
        "logged_model_uri = f\"runs:/{RUN_ID}/mt_model\"\n",
        "print(\"Loading model from MLflow:\", logged_model_uri)\n",
        "\n",
        "model = mlflow.pytorch.load_model(logged_model_uri)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ---------------- TRANSLATION FUNCTION ---------------- #\n",
        "def translate_english_to_bangla(sentence: str) -> str:\n",
        "    input_ids = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_tokens = model.model.generate(\n",
        "            input_ids,\n",
        "            max_length=MAX_LENGTH,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "# ---------------- GRADIO INTERFACE ---------------- #\n",
        "gr_interface = gr.Interface(\n",
        "    fn=translate_english_to_bangla,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Enter English sentence here...\", label=\"English Text\"),\n",
        "    outputs=gr.Textbox(label=\"Bangla Translation\"),\n",
        "    title=\"English ‚Üí Bangla Translator (from MLflow)\",\n",
        "    description=\"Translates English into Bangla using model loaded directly from MLflow.\"\n",
        ")\n",
        "\n",
        "# ---------------- LAUNCH ---------------- #\n",
        "gr_interface.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}